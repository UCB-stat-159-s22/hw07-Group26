{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac5b6a07-76c1-49c3-baa5-9eb94c734ee7",
   "metadata": {},
   "source": [
    "# Breast Cancer Wisconsin (Diagnostic) Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb878a04-54fe-4ec6-ad29-b2a927e81d7c",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4097725-28fa-47f8-8401-317615bcab5a",
   "metadata": {},
   "source": [
    "For our project, we as a group wanted to focus on the health industry. We wanted to find a dataset where we could apply predictions to give a diagnosis to the patient. As we were browsing for datasets, we had to decide which disease we wanted to study. We decided to look at cancer, as cancer is a widely studied disease today. This motivated us to analyze the breast cancer dataset publicly available on Kaggle. We believe that our analysis of this dataset would be helpful to doctors and patients, who want to find out whether the cancer is benign or malignant. \n",
    "\n",
    "For our model, we decided to choose to use a classification/prediction model as our goal is to classify whether the cancer is benign or malignant. To do this, we ran different models including logistic regression, decision trees and random forests and then, based on the performance on the vailidation set, we picked the best one. After that, we evaluated our final model on the test set. Furthermore, we looked at the significance of our model parameters using confidence intervals and performede hypothesis testing using the parametric Two-Sample T-Test and the non-parametric Wilcoxian Rank Sumt Test to see whether our results are statistically significant at a significance level of 5%. We would like the results of our hypothesis testing to serve as initial indicators of potentical cancer diagnosis for the doctors. Based on our final model, we hope that our model can serve as a strong basis for predicting whether the cancer is benign or malignant based on its characteristics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d11287-6c59-457c-9b62-39db1cfd475c",
   "metadata": {},
   "source": [
    "## Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd91ced-7134-4972-a3bb-e59b3b26abe1",
   "metadata": {},
   "source": [
    "The raw data is from Kaggle, which can be found [here](https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data). The data was given to us in a csv file with 569 observations, so it was hard to interpret. To better interpret the data, we converted the data into a pandas dataframe. Each row of the data represents a specific patient. 357 patients have benign cancer and 212 have malignant cancer. The dataset contains 32 columns with 30 features representing the tumor's characteristics. We treated the diagnosis column as our response variable. All of the features are quantitative (float64) except for the id column (int64) and the diagnosis ('M' or 'B').  To combat this issue, we One Hot Encoded the diagnosis to convert it to 1s and 0s, which would allow us to run our classification model. The diagnosis is read as follows: 0 for benign and 1 for malignant. We also dropped the last column as it was empty. \n",
    "\n",
    "For each patient, a digitized image depicting the fine needle aspirate (FNA) of a breast mass is taken. Ten real-valued features are then computed for each cell nuclei present in the image. To summarize the findings for each patient, the table below contains the mean, the standard error, and the \"worst\" (mean of the three largest values) for each of the ten features, generating a total of thirty features for each patient.\n",
    "\n",
    "Below is the cleaned dataset that we generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99d8caeb-c866-4180-a4d6-9b47e60f5166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302        1.0        17.99         10.38          122.80     1001.0   \n",
       "1    842517        1.0        20.57         17.77          132.90     1326.0   \n",
       "2  84300903        1.0        19.69         21.25          130.00     1203.0   \n",
       "3  84348301        1.0        11.42         20.38           77.58      386.1   \n",
       "4  84358402        1.0        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
       "0  ...         25.38          17.33           184.60      2019.0   \n",
       "1  ...         24.99          23.41           158.80      1956.0   \n",
       "2  ...         23.57          25.53           152.50      1709.0   \n",
       "3  ...         14.91          26.50            98.87       567.7   \n",
       "4  ...         22.54          16.67           152.20      1575.0   \n",
       "\n",
       "   smoothness_worst  compactness_worst  concavity_worst  concave points_worst  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   symmetry_worst  fractal_dimension_worst  \n",
       "0          0.4601                  0.11890  \n",
       "1          0.2750                  0.08902  \n",
       "2          0.3613                  0.08758  \n",
       "3          0.6638                  0.17300  \n",
       "4          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "data = pd.read_csv('data/clean.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed046b6c-2478-441e-ae26-0f0591e1337d",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6680b50d-ba3c-495f-b729-a2d8838460e0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8456d5c-2e16-493d-aaae-8c8f3790b68e",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e56b782-d128-4b3b-8d84-22597d7ec60f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acd0ae95-5fab-4092-9254-9ae986d7b924",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3bd901-4306-4e86-b346-9d19f200ed71",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "852c35e8-4aa0-4406-be3d-11505f6a7f1e",
   "metadata": {},
   "source": [
    "### Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a350f8-2f53-46b7-abd1-285bf2567a6a",
   "metadata": {},
   "source": [
    "In this section, we will perform logistic regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46b6c75-7a78-4c9a-902f-2ce1afba3882",
   "metadata": {},
   "source": [
    "Description: Logistic Regression works by calculating posterior probabilies for the data by using the logistic function and based on those probablities it classifies the data point.Given below is the logistic function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7f6e88-dd5f-47d3-b2ba-cbc3f7dd4227",
   "metadata": {},
   "source": [
    "$$ s(x) = \\frac{1}{1+ e^{-x}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125598e2-783d-4677-8687-0ce97c2b1c1c",
   "metadata": {},
   "source": [
    "Limitations and Assumptions: An important thing to note about logistic regression is that it can only be used for binary classifiation and not multiclass classification. This is because it follows the rule that any point whose probability of being in a class is more than 50% is assigned to that class. If not, it is assigned to the other class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5ff5fc-3477-4d0d-8bb6-73aa82bf2561",
   "metadata": {},
   "source": [
    "Preprocessing: For the preprocessing, we dropped the diagnosis column (since we don't want to see the labels) and the id column (since it is not a very predictive feature) to create our feature matrix. Then we evaluated the model using various metrics. The calculated metrics and the preprocessing can be found in the logistic_reg.ipynb. Here are the plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "655c97ac-c104-4ff2-b43e-896c83138cb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'figures/confusion_matrix_logistic.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m conf_image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfigures/confusion_matrix_logistic.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m prec_recall_image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfigures/precision_recall_curve_logistic.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m roc_curve_image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfigures/roc_curve_logistic.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/PIL/Image.py:2975\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2972\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m   2974\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 2975\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2976\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2978\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'figures/confusion_matrix_logistic.png'"
     ]
    }
   ],
   "source": [
    "conf_image = Image.open('figures/confusion_matrix_logistic.png')\n",
    "prec_recall_image = Image.open('figures/precision_recall_curve_logistic.png')\n",
    "roc_curve_image = Image.open('figures/roc_curve_logistic.png')\n",
    "display(conf_image)\n",
    "display(prec_recall_image)\n",
    "display(roc_curve_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd740f0-9a96-4bc6-9950-cd5317df90d8",
   "metadata": {},
   "source": [
    "Evaluation: The classifier seems to performing well with a very few false positive (falsely classifying cancer as malignant) and false negatives (falsely classfying the cancer as benign). In addition, the AUC of the classifier is 0.92, which means that the classifier is getting the correct answer 92% of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51645001-45b7-4d1a-a1a8-753734902b3d",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dff0b2-142a-4e8a-87ba-327f524d7bfc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b575396-b2dc-4da2-8f27-3ed3e3fb089a",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1f8fce-ba3b-4d4d-b572-bfb45ba9c392",
   "metadata": {},
   "source": [
    "## Assess significance of features using LR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f362113-60e4-4065-b128-4772d05cc35d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "821da7ad-e9af-4cb1-81a9-a63e232dd2be",
   "metadata": {},
   "source": [
    "## Hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef63e549-29b5-4ab3-b2a4-465bc0122229",
   "metadata": {},
   "source": [
    "While classification helps to distinguish malignant breast cancer patients from benign cancer patients, we next want to see whether some features differ in the two populations which may not be attributed to just chance due to the sample observed. More specifically,  using parametric and non-parametric hypothesis testing, we will identify features that have a significantly different average value in the two populations, which doctors may then utilize as initial indicators of malignant vs. benign cancer prior to actual diagnosis. Rather than immediately identifying patiients with either malignant or benign cancer, which may require health senstive and cost intensive procedures, doctors may use the results of the hypothesis testing to better guide the patients in the early stages of their treatments. Additionally, if all of the required input features are not present to use the models we developed above, doctors may utilize the results of hypothesis testing.   \n",
    "\n",
    "We identified that the 'area worst' feature, which is the mean of the three highest values of the area feature for each patient, had the largest average difference between the two populations and a large difference in their sample variances as well. On the otherhand, the 'texture se', which is the standard error of the texture feature, had the smallest average difference between the two populations as well as close sample variances. We specifically utilized the Two Sample T Test and the Wilcoxon Rank Sum Test to conduct the hypothesis testings, as the assumption of normality for the two samples may not be assumed with high confidence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72de23fa-f04c-4cb9-bc82-598f53324dd1",
   "metadata": {},
   "source": [
    "For each of the two features identified, we performed the following hypothesis test (parametric Two Sample T Test and the non-parametric Wilcoxian Rank Sum Test).\n",
    "$$H_0: \\mu_0 = \\mu_1$$\n",
    "\n",
    "$$H_1: \\mu_0 \\neq \\mu_1$$\n",
    "\n",
    "In the table below, we see that the 'area worst' feature has a statistically highly significant p value for both the parametric and the non-parametric test, which is less than 1%. Hence, we reject the null hypothesis that the two populations have the same mean value for the 'area worst' feature. On the otherhand, for the 'texture se' feature, we see that the p value is above 5% for both the parametric and the non-parametric test, hence we fail to reject the null hypothesis that the two populations have the same mean value for the 'texture se' feature. The doctors may choose to utilize thee 'area worst' faeture as an early pontential indicator for malignant vs. benign cancer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "220f30c0-969e-4453-a41d-acb75183e1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T Statistic</th>\n",
       "      <th>P Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>area worst parametric</th>\n",
       "      <td>-20.570814</td>\n",
       "      <td>4.937924e-54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area worst non-parametric</th>\n",
       "      <td>-18.754029</td>\n",
       "      <td>1.794645e-78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture se parametric</th>\n",
       "      <td>0.197724</td>\n",
       "      <td>8.433320e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture se non-parametric</th>\n",
       "      <td>-0.462805</td>\n",
       "      <td>6.435040e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           T Statistic       P Value\n",
       "area worst parametric       -20.570814  4.937924e-54\n",
       "area worst non-parametric   -18.754029  1.794645e-78\n",
       "texture se parametric         0.197724  8.433320e-01\n",
       "texture se non-parametric    -0.462805  6.435040e-01"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht_results = pd.read_pickle(\"tables/ht_results.pkl\")\n",
    "ht_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4132f1-b137-4389-8a3a-53e28294c7e7",
   "metadata": {},
   "source": [
    "## Discussion & Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e09e2a-2777-49ad-be2e-4a247b42c639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c4c4a68-aa7b-4103-9709-16066bfb1744",
   "metadata": {},
   "source": [
    "## Author Contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01884d69-df65-4ae3-b228-2689f3516e6d",
   "metadata": {},
   "source": [
    "Kshitij Chauhan (TJ): Created the logistic_reg.ipynb notebook to conduct logistic regression analysis and tests in test_logistic_reg.py file to test the functions. Also did the introduction, data description and logistic regression sections of this notebook.\n",
    "\n",
    "Neha Haq: Created the two_populations_analysis.ipynb notebook to conduct the parametric Two Sample T Tests and the non-parametric Wilcoxon Rank Sum Test. Set up the diagnosis python package and wrote the methods in twosample.py and the tests in the test_twosample.py. Created the Jupyter Book and the Github actions. Wrote the hypothesis tests section of the main.ipynb. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74575b4-f49a-4948-aaa2-ff514fdd06dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw07",
   "language": "python",
   "name": "hw07"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
