{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faf36848",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "In this notebook, we prepare the dataset for Exploratory Data Analysis (EDA), model construction and selection, and model evaluation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8c1cbf",
   "metadata": {},
   "source": [
    "## Import the packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6772fa6",
   "metadata": {},
   "source": [
    "Import all the necessary packages for the following analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59a0cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f8de7c",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b62044",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv('../data/raw_data.csv')\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cfdb59",
   "metadata": {},
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83053f7",
   "metadata": {},
   "source": [
    "Drop the empty column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cb2d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the last column\n",
    "clean = raw.iloc[:, :-1]\n",
    "clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3596ef2b",
   "metadata": {},
   "source": [
    "Check and fix the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaa2d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the missing values in each column\n",
    "clean.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633c04b6",
   "metadata": {},
   "source": [
    "Check and drop the duplicate observations/rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee55b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the duplicated observations/rows\n",
    "clean.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d5202b",
   "metadata": {},
   "source": [
    "Check and fix the data type of columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f004001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the type of each column\n",
    "clean.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6235d9",
   "metadata": {},
   "source": [
    "Since `diagnosis` is a categorical variable, we need to one-hot encode it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac50d201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a one hot encoder and use it to transform the data\n",
    "encoder = OneHotEncoder(drop='first') # drop one category to avoid potential singularity\n",
    "diagnosis_enc = encoder.fit_transform(clean[['diagnosis']]).toarray()\n",
    "enc_clean = clean.copy()\n",
    "enc_clean['diagnosis'] = diagnosis_enc\n",
    "enc_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff85f30",
   "metadata": {},
   "source": [
    "After the transformation, `diagnosis = 1.0` stands for `malignant` and `diagnosis = 0.0` stands for `benign`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d033b62",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train-val-test Split "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f67a13",
   "metadata": {},
   "source": [
    "First split the dataset into training+validation and testing sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c0fbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val, test = train_test_split(enc_clean, test_size=0.15, random_state=159)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f798dc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(train_val, test_size=0.2, random_state=159)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecd67d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape[0], val.shape[0], test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5725d1",
   "metadata": {},
   "source": [
    "## Save the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c35d66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_clean.to_csv('../data/clean.csv', index=False)\n",
    "train.to_csv('../data/train.csv', index=False)\n",
    "val.to_csv('../data/val.csv', index=False)\n",
    "test.to_csv('../data/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baf9fae",
   "metadata": {},
   "source": [
    "## Combine the pipeline\n",
    "We can combine the pipeline above into helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5162b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def clean_data(data, enc_columns=[]):\n",
    "    \"\"\"\n",
    "    Clean the data and one-hot encode the columns/features\n",
    "    ---\n",
    "    Arguments:\n",
    "    data (pandas.DataFrame): the data to clean\n",
    "    enc_columns (list[str]): the names of the columns to be one-hot encoded \n",
    "    \n",
    "    Returns:\n",
    "    clean_data (pandas.DataFrame): the cleaned data\n",
    "    \"\"\"\n",
    "    \n",
    "    # drop the last column\n",
    "    clean_data = data.iloc[:, :-1]\n",
    "    \n",
    "    # drop the observations with missing values\n",
    "    clean_data = clean_data.dropna()\n",
    "    \n",
    "    # drop the duplicated observations\n",
    "    clean_data = clean_data.drop_duplicates()\n",
    "    \n",
    "    # one-hot encode the columns\n",
    "    enc_clean = clean_data.copy()\n",
    "    for col in enc_columns:\n",
    "        encoder = OneHotEncoder(drop='first') # drop the first category to avoid singularity\n",
    "        diagnosis_enc = encoder.fit_transform(clean_data[[col]]).toarray()\n",
    "        enc_clean = clean_data.copy()\n",
    "        enc_clean[col] = diagnosis_enc\n",
    "    \n",
    "    clean_data = enc_clean.reset_index().drop('index', axis=1)\n",
    "    \n",
    "    return clean_data\n",
    "\n",
    "def load_data(file_path, enc_columns=[], val_size=0.2, test_size=0.15, random_state=159):\n",
    "    \"\"\"\n",
    "    Read, clean, and split the breast cancer data. Then save the edited datasets.\n",
    "    ---\n",
    "    Arguments:\n",
    "    file_path (str): the path to the data we want to load\n",
    "    enc_columns (list[str]): the names of the columns to be one-hot encoded \n",
    "    val_size (float): the percentage of the validation set among train+val sets\n",
    "    test_size (float): the percentage of the validation set among test\n",
    "    \n",
    "    Returns:\n",
    "    (train, val, test) (tuple(pandas.DataFrame * 3)): the training, validation, and testing sets\n",
    "    \"\"\"\n",
    "    assert (val_size <= 1) and (val_size >= 0), \"Invalid validation set size\" \n",
    "    assert (test_size <= 1) and (test_size >= 0), \"Invalid testing set size\"\n",
    "    \n",
    "    raw = pd.read_csv(file_path)\n",
    "    clean = clean_data(raw, enc_columns)\n",
    "    \n",
    "    # train-val-test split of the data\n",
    "    train_val, test = train_test_split(clean, test_size=test_size, random_state=random_state)\n",
    "    train, val = train_test_split(train_val, test_size=val_size, random_state=random_state)\n",
    "    \n",
    "    clean.to_csv('../data/clean.csv', index=False)\n",
    "    train.to_csv('../data/train.csv', index=False)\n",
    "    val.to_csv('../data/val.csv', index=False)\n",
    "    test.to_csv('../data/test.csv', index=False)\n",
    "    \n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98747e3e",
   "metadata": {},
   "source": [
    "Testing functions for the helper functions above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8484ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_clean():\n",
    "    test_input = pd.DataFrame(data={\"a\":[1.0, np.nan, 3.0, 1.0],\n",
    "                                    \"b\":[1.0, 3.0, 3.0, 1.0],\n",
    "                                    \"c\":[2.0, 4, 1, 4]})\n",
    "    test_output = pd.DataFrame(data={\"a\":[1.0, 3.0],\n",
    "                                     \"b\":[1.0, 3.0]})\n",
    "    out = clean_data(test_input)\n",
    "    assert test_output.equals(out)\n",
    "    \n",
    "def test_load():\n",
    "    return\n",
    "\n",
    "test_clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d23a26b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "hw07"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "ligohw07"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    },
    {
     "key": "language_info",
     "op": "remove"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "python3"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    },
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "3.8.6"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
